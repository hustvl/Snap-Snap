
<DOCTYPE html>

    <title>Snap-Snap: Taking Two Views to Reconstruct Human 3D Gaussians in Milliseconds</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-21408087-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-21408087-2');
    </script>

    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="./static/css/style.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

    <style>
        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fCBc4EsA.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBxc4EsA.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fChc4EsA.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBBc4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7mxKOzY.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4WxKOzY.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7GxKOzY.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4mxK.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }
    </style>

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <body>
        <div class="container">
            <div class="row mb-2 mt-4" id="paper-title">
                <h1 class="col-md-12 text-center">
                    Snap-Snap
                </h1>
                <h3 class="col-md-12 text-center">
                    Taking Two Views to Reconstruct Human 3D Gaussians in Milliseconds
                </h3>
            </div>

            <div class="row" id="authors">
                <div class="mx-auto text-center">
                    <ul class="list-inline mb-0">
                        <li class="list-inline-item">
                            <a>Jia Lu&#42;</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://taoranyi.com/">Taoran Yi&#42;</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://jaminfong.cn/">Jiemin Fang</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="https://scholar.google.com/citations?hl=zh-CN&user=StdXTR8AAAAJ">Chen Yang</a><sup>3</sup>

                        <li class="list-inline-item">
                            <a>Chuiyun Wu</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://shenwei1231.github.io/">Wei Shen</a><sup>3</sup>

                        <li class="list-inline-item">
                            <a href="http://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://www.qitian1987.com/">Qi Tian</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="https://xwcv.github.io/">Xinggang Wang</a><sup>1</sup>

                    </ul>
                    <p id="institution">
                        <sup>1</sup>Huazhong University of Science and Technology &nbsp; &nbsp;
                        <sup>2</sup>Huawei Inc. &nbsp; &nbsp;
                        <sup>3</sup>Shanghai Jiaotong University &nbsp; &nbsp;
                    </p>
                    <p id="equal-contribution">
                    &#42; Equal Contribution
                    </p>
                </div>
            </div>
            <div class="row mb-2" id="links">
                <div class="mx-auto">
                    <ul class="nav">
                        <li class="nav-item text-center">
                            <a href="https://arxiv.org" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                                </svg><br>
                                Paper
                            </a>
                        <li class="nav-item text-center">
                            <a href="https://github.com/hustvl/Snap-Snap" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z" />
                                </svg><br>
                                Code
                            </a>
                    </ul>
                </div>
            </div>

            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">

                    <div id="dynamic-teaser">

                        <h6 style="color:#8899a5"> TL;DR: Snap-Snap can directly predict 3D human Gaussians from just two images in milliseconds without human prior.</h6>
                        <div class="col-12 text-center">

                            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                                <source src="assets/videos/teaser.mp4" type="video/mp4">
                            </video>
                        </div>

                    </div> <!-- dynamic-teaser -->
                </div>
            </div>

            <div class="row mb-3">
                <div class="col-md-8 mx-auto">
                    <h4 class="pb-2">Abstract</h4>
                    <p class="text-justify">
                        Reconstructing 3D human bodies from sparse views has been an appealing topic, which is crucial to broader the related applications. In this paper, we propose a quite challenging but valuable task to reconstruct the human body from only two images, i.e., the front and back view, which can largely lower the barrier for users to create their own 3D digital humans. The main challenges lie in the difficulty of building 3D consistency and recovering missing information from the highly sparse input. We redesign a geometry reconstruction model based on foundation reconstruction models to predict consistent point clouds even input images have scarce overlaps with extensive human data training. Furthermore, an enhancement algorithm is applied to supplement the missing color information, and then the complete human point clouds with colors can be obtained, which are directly transformed into 3D Gaussians for better rendering quality. Experiments show that our method can reconstruct the entire human in 190 ms on a single NVIDIA RTX 4090, with two images at a resolution of 1024*1024, demonstrating state-of-the-art performance on the THuman2.0 and cross-domain datasets. Additionally, our method can complete human reconstruction even with images captured by low-cost mobile devices, reducing the requirements for data collection.
                    </p>
                    <img src="./assets/images/teaser_fig.png" class="img-responsive" alt="teaser" 
                        style="width:75%; display:block; margin:0 auto;">
                </div>
            </div>


            <div class="row mb-4" id="overview">
                <div class="col-md-8 mx-auto grey-container">
                    <h4 class="pb-2">Overview</h4>
                    <p class="text-justify">
                    (a) We design a feed-forward reconstruction framework, which can directly predict 3D human Gaussians from just two images in milliseconds without human prior.
                    (b) We redesign a geometry reconstruction model which can build human point clouds even with highly sparse input, adapting the generalizable geometric prior to the human domain. In addition, a side view enhancement algorithm is proposed to supplement the unseen information.
                    (c) With two-view images at a resolution of 1024*1024, our method can obtain complete human reconstruction results in 190 ms and demonstrates state-of-the-art performance on the THuman2.0 and cross-domain datasets. The method is also shown to perform well on data acquired from low-cost mobile devices.
                    </p>
                    <img src="./assets/images/pipeline.png" class="img-responsive" alt="pipeline">
                </div>
            </div>

    <div class="row mb-3 pt-2">
        <div class="col-md-8 mx-auto">

        <div id="in-domain-thuman">
            <h4>In-domain Generalization - Tested on THuman 2.0</h4>
                            <p class="text-justify">
                                Without relying on any human-specific priors, Snap-Snap is able to reconstruct high-quality human bodies from only two input views. Even with just two views, our method can recover the complete human body while maintaining accurate alignment with the inputs, especially in clothing details and hair. Our method is trained and evaluated on the THuman2.0 dataset.
                            </p>
        <div class="slider-container" id="slider1">
            <div class="slider">
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="assets/videos/demo_thuman.mp4" type="video/mp4">
                </video>
            </div>
        </div>

    </div>
    </div>
    </div>

    <div class="row mb-3 pt-2">
        <div class="col-md-8 mx-auto">
    
        <div id="in-domain-loose">
            <h4>In-domain Generalization - Tested on THuman 2.1 Loose Set</h4>
                            <p class="text-justify">
                                To evaluate the reconstruction quality of loose clothing, we select 50 loose clothing examples from Thuman2.1 by calculating the chamfer distance between the SMPL-X mesh and the ground-truth human meshes. Our method directly infers complete geometric information through the point cloud prediction model, achieving good modeling even for loose clothing.
                            </p>
        <div class="slider-container" id="slider2">
            <div class="slider">
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="assets/videos/demo_loose.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    
        </div>
        </div>
        </div>

    <div class="row mb-3 pt-2">
    <div class="col-md-8 mx-auto">

    <div id="cross-domain-2k2k">
        <h4>Cross-domain Generalization - Tested on 2K2K</h4>
                        <p class="text-justify">
                            We conduct cross-domain evaluation on the 2K2K dataset with the model trained on THuman2.0 (426 scans). Our method can achieve efficient and high-quality human reconstruction even with fewer input views.
                        </p>
    <div class="slider-container">
        <div class="slider" id="slider3">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="assets/videos/demo_2k2k.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    </div>
    </div>
    </div>

    <div class="row mb-3 pt-2">
    <div class="col-md-8 mx-auto">

    <div id="cross-domain-4ddress">
        <h4>Cross-domain Generalization - Tested on 4D-Dress</h4>
                        <p class="text-justify">
                            We conduct cross-domain evaluation on the 4D-Dress dataset with the model trained on THuman2.0 (426 scans). Our method can achieve efficient and high-quality human reconstruction even with fewer input views.
                        </p>
    <div class="slider-container">
        <div class="slider" id="slider4">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="assets/videos/demo_dress.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    </div>
    </div>
    </div>

    <div class="row mb-3 pt-2">
    <div class="col-md-8 mx-auto">

    <!-- <div id="citation"> -->
        <!-- <h4 class="title">BibTeX</h4> -->
        <!-- <div class="container is-max-desktop content">

            <pre><code>
        N/A
            </code></pre>
          </div> -->

    <div class="container" style="max-width: 768px;">
          <footer>
              <p> Website template from <a href="https://github.com/humansensinglab/Generalizable-Human-Gaussians">GHG</a> and <a href="https://github.com/taoranyi/taoranyi.github.io/tree/main/gaussiandreamer">gaussiandreamer</a>. We thank the authors for the open-source code.</p>
          </footer>
    </div>


    </div> <!-- container -->
    </body>
